<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>README.md - Grip</title>
  <link rel="icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEACABoBQAAFgAAACgAAAAQAAAAIAAAAAEACAAAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAAKCgr/FRUX/xYWGP8YGBr/GRkb/xsbHP8dHR7/Hh4f/x4eIP8fHyD/ICAh/yMjJP8kJCX/JiYo/ygoKv8pKSr/LCwu/y0tLv80NDT/QUFC/0ZGR/9MTE7/UFBR/1JSUv9bW1v/XFxc/2BgYP9hYWP/aGhp/2xsbP9ycnL/d3d3/3l5e/98fHz/f3+A/2dn/P9z3a//cNX8/4CAgP+Xl5f/nZ2e/56en/+fn5//n5+h/6amp/+rq6v/rKys/7CvsP+zs7T/uLi5/7u7u/+8vLz/wMDA/8fHx//Kysr/zc3N/9HR0f/d3d3/3t7e/+Xl5f/q6ur/6+vr/+3t7f/z8/P/9fX1//b29v/39/f/+Pj4//n5+f/6+vr/+/v7//z8/P/9/f3//v7+//////8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/NCYmJiYmJiYmJiYmJiYmNCZKSkpKSkpKSkpKSkpKSiYmSkpKRisdRkMbMklKSkomJkpKSSEiNUhEEQwpSUpKJiZKSi8LHhhAPAwBDDZKSiYmSkoWACA7SEc6HAEaSkomJkpKBxI/SkpKSj4ICkpKJiZKSgUTR0pKSkpGDwZKSiYmSkoTED1KSkpKOQQVSkomJkpKKgowMy4sNycIMUpKJiZKSkEZDgkDAgsNH0VKSiYmSkpKQigUBgkXLUhKSkomJkpKSkpKSkpKSkpKSkpKJiYmJiYmJiYmJiYmJiYmJiYmIyUkNDQ0NDQ0NDQ0NDQmOCYmJiYmJiYmJiYmJiYmOAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=" />
  <link rel="stylesheet" href="//octicons.github.com/components/octicons/octicons/octicons.css" />
  <style>
    /* Page tweaks */
    .preview-page {
      margin-top: 64px;
    }
    /* User-content tweaks */
    .timeline-comment-wrapper > .timeline-comment:after,
    .timeline-comment-wrapper > .timeline-comment:before {
      content: none;
    }
    /* User-content overrides */
    .discussion-timeline.wide {
      width: 920px;
    }
  </style>
</head>
<body>
  <div class="page">
    <div id="preview-page" class="preview-page" data-autorefresh-url="">

    

      <div role="main" class="main-content">
        <div class="container new-discussion-timeline experiment-repo-nav">
          <div class="repository-content">
            <div id="readme" class="readme boxed-group clearfix announce instapaper_body md">
              
                <h3>
                  <span class="octicon octicon-book"></span>
                  README.md - Grip
                </h3>
              
              <article class="markdown-body entry-content" itemprop="text" id="grip-content">
                <h1>
<a id="user-content-duplex-sequencing-pipeline" class="anchor" href="#duplex-sequencing-pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Duplex Sequencing Pipeline</h1>
<p>Brendan Kohrn, March 30, 2020<br>
Duplex Sequencing is copyright Scott Kennedy and Larwrence Loeb,
University of Washington, Seattle, WA, USA</p>
<h2>
<a id="user-content-table-of-contents" class="anchor" href="#table-of-contents" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Table of Contents:</h2>
<ol>
<li>Glossary</li>
<li>Dependencies</li>
<li>Setup</li>
<li>Genome Setup</li>
<li>Contaminant database setup</li>
<li>Bed and Interval_list file preparation</li>
<li>Configuration File creation</li>
<li>Recovery script creation</li>
<li>Running pipelines</li>
<li>Output file description</li>
<li>Testing the pipeline</li>
<li>Full and partial reruns</li>
</ol>
<h2>
<a id="user-content-1-glossary" class="anchor" href="#1-glossary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1: Glossary</h2>
<h3>
<a id="user-content-single-stranded-consensus-sequence-sscs" class="anchor" href="#single-stranded-consensus-sequence-sscs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Single Stranded Consensus Sequence (SSCS)</h3>
<p>A construct created by comparing multiple reads and deciding ambiguities by
simple majority.</p>
<h3>
<a id="user-content-duplex-consensus-sequence-dcs" class="anchor" href="#duplex-consensus-sequence-dcs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Duplex Consensus Sequence (DCS)</h3>
<p>A construct created by comparing two SSCSs.</p>
<h3>
<a id="user-content-family" class="anchor" href="#family" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Family</h3>
<p>A group of reads that shares the same tag sequence.</p>
<h2>
<a id="user-content-2-dependencies" class="anchor" href="#2-dependencies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2: Dependencies:</h2>
<p>This pipeline is known to work with the following minimum versions of the follow required programs:</p>
<ul>
<li>Python3.6+</li>
<li>Snakemake=5.5.*</li>
<li>Pandas</li>
<li>Miniconda/Anaconda=4.7.*</li>
<li>A GATK3.8.1 .jar file</li>
<li>bwa=0.7.17.* (for genome setup)</li>
<li>ncbi-blast=&gt;2.6.0 (installed separately, for contaminant database setup)</li>
<li>wget (on macOS, install using homebrew; present by default on linux)</li>
</ul>
<p>Once Python3.6 is installed, snakemake and pandas can be installed using
pip3 or using whatever package manager you're using.  GATK3.8.1 can be
downloaded from
<a href="https://console.cloud.google.com/storage/browser/_details/gatk-software/package-archive/gatk/GenomeAnalysisTK-3.8-1-0-gf15c1c3ef.tar.bz2" rel="nofollow">https://console.cloud.google.com/storage/browser/_details/gatk-software/package-archive/gatk/GenomeAnalysisTK-3.8-1-0-gf15c1c3ef.tar.bz2</a>.<br>
<strong>We need to use GATK3.8.1 since it is the last version of GATK that
includes the IndelRealigner functionality; this part was removed in
GATK4</strong>.  Blast can be downloaded in any of several ways, including some
package managers (Ubuntu: sudo apt-get install ncbi-blast+).  It can
also be installed using conda if desired.</p>
<h2>
<a id="user-content-3-setup" class="anchor" href="#3-setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3: Setup:</h2>
<p>Find the location where you want the pipeline to be located and clone the pipeline using git:</p>
<div class="highlight highlight-source-shell"><pre>git clone https://github.com/KennedyLabUW/Duplex-Seq-Pipeline.git</pre></div>
<p>change into the directory, and run:</p>
<div class="highlight highlight-source-shell"><pre>bash setupDS.sh GATK_JAR_PATH MAX_CORES</pre></div>
<p>where GATK_JAR_PATH is the path to your pre-downloaded GATK jar, and MAX_CORES is the maximum number of cores you want the pipeline to be able to use. After this, with the exception of setting up the Genomes (See Section 4) and the (optional) blast contamination database (See Section 5), you should be able to run the Duplex-Seq pipeline using</p>
<div class="highlight highlight-source-shell"><pre>DS CONFIG_CSV.csv</pre></div>
<p>where CONFIG_CSV.csv is a configuration CSV file generated as described below.</p>
<h2>
<a id="user-content-4-genome-setup" class="anchor" href="#4-genome-setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4: Genome setup</h2>
<p>Put genomes in an easily findable location, such as our references directory (i.e. ~/bioinformatics/reference).</p>
<p>Many genomes can be downloaded from UCSC (<a href="http://hgdownload.soe.ucsc.edu/downloads.html" rel="nofollow">http://hgdownload.soe.ucsc.edu/downloads.html</a>).  In order to download genomes from there, you will need to download the twoBitToFa program from the appropriate Utilities directory.  twoBitToFa has the following syntax (Copied from UCSC):</p>
<pre><code>twoBitToFa - Convert all or part of .2bit file to fasta
usage:
   twoBitToFa input.2bit output.fa
options:
   -seq=name       Restrict this to just one sequence.
   -start=X        Start at given position in sequence (zero-based).
   -end=X          End at given position in sequence (non-inclusive).
   -seqList=file   File containing list of the desired sequence names 
                   in the format seqSpec[:start-end], e.g. chr1 or chr1:0-189
                   where coordinates are half-open zero-based, i.e. [start,end).
   -noMask         Convert sequence to all upper case.
   -bpt=index.bpt  Use bpt index instead of built-in one.
   -bed=input.bed  Grab sequences specified by input.bed. Will exclude introns.
   -bedPos         With -bed, use chrom:start-end as the fasta ID in output.fa.
   -udcDir=/dir/to/cache  Place to put cache for remote bigBed/bigWigs.

Sequence and range may also be specified as part of the input file name using the syntax:

      /path/input.2bit:name
   or
      /path/input.2bit:name
   or
      /path/input.2bit:name:start-end
</code></pre>
<p>Once you have downloaded a genome and converted it into FASTA format, it needs to be indexed.  To do this, open a terminal and navigate to your the directory containing your genome.  Note that in the following commands, the word "genome.fasta" should be replaced with the file name of your genome. These commands <strong>must</strong> be run in the same directory as the reference genome</p>
<pre><code>    bwa index genome.fasta  
    samtools faidx genome.fasta  
    # /Path/To/PicardTools should be the path to wherever you put your picard tools jar file.    
    # Also, genome.dict should match the name of your fasta (e.g. hg19.fasta to hg19.dict)  
    java -jar /Path/To/PicardTools/picard.jar CreateSequenceDictionary R=genome.fasta O=genome.dict  
</code></pre>
<h2>
<a id="user-content-5-contaminant-database-setup" class="anchor" href="#5-contaminant-database-setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5: Contaminant Database Setup:</h2>
<p>The Duplex-Seq pipeline is designed to use a local NCBI Blast instance to detect and remove potential contamination from non-target species and identify issues arising from pseudogenes.
<em>This step is optional, but requires a non valid NCBI Blast database file name. We recommend either a dummy path and file name or '.' to be present in the blast_db field in the config.csv file. Blank entries will result in pipeline failure.</em></p>
<p>To construct your contaminant database, if desired, first decide on a
list of species you want to monitor for contaminants.  A suggested
starting list is:</p>
<ul>
<li>Human (hg38)</li>
<li>Mouse (mm10)</li>
<li>Rat (rn6)</li>
<li>C. elegens (ce11)</li>
<li>Yeast (sacCer3)</li>
<li>Fruit Fly (dm6)</li>
<li>Cow (bosTau9)</li>
<li>Dog (canFam3)</li>
</ul>
<p><strong>It is important that any genome you plan to use for alignment be included in this database,
in the same version (e.g. if your alignment genome uses UCSC chromosome
names, your genome in the database cannot use NCBI chromosme names, but
must also use UCSC chromosome names)</strong>.</p>
<p>Database setup consists of three steps:</p>
<ol>
<li>Genome labeling:</li>
</ol>
<p>Label each record in the genome with the ncbi taxID for the species the
genome is associated with.  TaxIDs can be found using the NCBI taxonomy
website (<a href="https://www.ncbi.nlm.nih.gov/taxonomy" rel="nofollow">https://www.ncbi.nlm.nih.gov/taxonomy</a>).  This is done by running:</p>
<pre><code>python3 AddTaxonID.py GENOME.fa TAXID GENOME_taxID.fa
</code></pre>
<p>where GENOME.fa is the input fasta file with the genome, TAXID is the NCBI taxonomy ID for the species associated with the genome, and GENOME_taxID.fa is the output labeled genome.</p>
<ol start="2">
<li>Sub-database Creation:</li>
</ol>
<p>Create the database using:</p>
<pre><code>makeblastdb \  
-dbtype nucl \  
-title GENOME \  
-out GENOME_db \  
-in GENOME_taxID.fa  
</code></pre>
<p>After this, create a .nal file for this database following this template:</p>
<pre><code>#GENOME.nal 
TITLE GENOME
DBLIST GENOME_db
</code></pre>
<p>The blastDbSetup.sh script can be used to automate these steps.  It can
be run with:</p>
<pre><code>bash /path/to/pipeline/setupBlastDb/blastDbSetup.sh GENOME.fa TAXON_ID
</code></pre>
<ol start="3">
<li>Full database creation:</li>
</ol>
<p>Once you've created all your sub-databases (e.g. GENOME1, GENOME2,
GENOME3, ..., GENOME_N), create a .nal file to represent the full
database following this template:</p>
<pre><code>#contaminantDb.nal 
TITLE contaminantDb
DBLIST GENOME1_db GENOME2_db GENOME3_db ... GENOME_N_db
</code></pre>
<p>If, at a later time, you need to change your contaminant database, you
then only need to rebuild the modified portion, and then update this
file to reflect that.</p>
<h2>
<a id="user-content-6-bed--file-preparation" class="anchor" href="#6-bed--file-preparation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>6: Bed  file preparation</h2>
<p>A bed file is a file which details regions of the genome in which you
are interested.  The syntax for bed files is described at
<a href="https://genome.ucsc.edu/FAQ/FAQformat.html#format1" rel="nofollow">https://genome.ucsc.edu/FAQ/FAQformat.html#format1</a>.</p>
<p>If you know which genes you are targeting and are using a common
published genome, the bed file can be downloaded from the UCSC Table
Browser (<a href="https://genome.ucsc.edu/cgi-bin/hgTables" rel="nofollow">https://genome.ucsc.edu/cgi-bin/hgTables</a>).  Otherwise, it can
be created using results from a BLAST search or any other method you
like.  <strong>This pipeline does not currently support overlapping intervals,
does support blocks as described in the bed spec.</strong></p>
<h2>
<a id="user-content-7-configuration-file-creation" class="anchor" href="#7-configuration-file-creation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>7: Configuration file creation:</h2>
<p>Use the ConfigTemplate to create a new file with the appropriate headers.  For each row, fill in the information about a particular sample:</p>
<table>
<thead>
<tr>
<th>Header</th>
<th>Information</th>
</tr>
</thead>
<tbody>
<tr>
<td>sample</td>
<td>A unique identifier for a sample; this will be used to name all output files for this sample</td>
</tr>
<tr>
<td>rglb</td>
<td>Read Group Library Identifier</td>
</tr>
<tr>
<td>rgpl</td>
<td>Read Group Platform; usually illumina</td>
</tr>
<tr>
<td>rgpu</td>
<td>Read Group Platform Unit</td>
</tr>
<tr>
<td>rgsm</td>
<td>Read Group Sample</td>
</tr>
<tr>
<td>reference</td>
<td>The path to the prepared reference genome to use with this sample.</td>
</tr>
<tr>
<td>target_bed</td>
<td>A bed file showing where the targets are for this particular sample</td>
</tr>
<tr>
<td>blast_db</td>
<td>The blast database to use for contaminant filtering; must include your target genome.</td>
</tr>
<tr>
<td>targetTaxonId</td>
<td>The taxon ID of the species you are expecting to be present in the sample.</td>
</tr>
<tr>
<td>baseDir</td>
<td>The directory the input files are in, and where the output files will be created.</td>
</tr>
<tr>
<td>in1</td>
<td>The read1 fastq (or fastq.gz, or fq.gz, or fq) file for this sample. Note that this is just the name of the file, and not the full path.</td>
</tr>
<tr>
<td>in2</td>
<td>The read2 fastq (or fastq.gz, or fq.gz, or fq) file for this sample. Note that this is just the name of the file, and not the full path.</td>
</tr>
<tr>
<td>mqFilt</td>
<td>A threshold for mapping quality filtering, if desired.</td>
</tr>
<tr>
<td>minMem</td>
<td>The minimum number of reads that must be in a family for consensus making</td>
</tr>
<tr>
<td>maxMem</td>
<td>The maximum number of reads in a family the consensus maker should consider.</td>
</tr>
<tr>
<td>cutOff</td>
<td>The threshold for consensus making; the consensus maker will require at least this much agreement on a per base pair level.</td>
</tr>
<tr>
<td>nCutOff</td>
<td>The maximum proportion of N bases in an output consensus sequence.</td>
</tr>
<tr>
<td>umiLen</td>
<td>The length of the UMI in this sample</td>
</tr>
<tr>
<td>spacerLen</td>
<td>The length of the spacer sequence in this sample</td>
</tr>
<tr>
<td>locLen</td>
<td>The localization length to use for this sample</td>
</tr>
<tr>
<td>readLen</td>
<td>The length of a read for this sample</td>
</tr>
<tr>
<td>clipBegin</td>
<td>How many bases to clip off the 5' end of the read</td>
</tr>
<tr>
<td>clipEnd</td>
<td>How many bases to clip off the 3' end of the read</td>
</tr>
<tr>
<td>minClonal</td>
<td>The minimum clonality to use for count_muts generation</td>
</tr>
<tr>
<td>maxClonal</td>
<td>The maximum clonality to use for count_muts generation</td>
</tr>
<tr>
<td>minDepth</td>
<td>The minimum depth to use for count_muts generation</td>
</tr>
<tr>
<td>maxNs</td>
<td>The maximum proportion of N bases to use for count_muts generation</td>
</tr>
<tr>
<td>runSSCS</td>
<td>True or False; whether to do full analysis for SSCS data.</td>
</tr>
<tr>
<td>recovery</td>
<td>The recovery script to use in attempting to recover ambiguously mapped reads (as determine by blast alignment vs bwa alignment).  Recovery script creation is discussed in 8; below.</td>
</tr>
</tbody>
</table>
<p>Save the file as a .csv file with unix line endings (LF).</p>
<h2>
<a id="user-content-8-recovery-script-creation" class="anchor" href="#8-recovery-script-creation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8: Recovery script creation</h2>
<p>As part of its operation, this pipeline filters out correct-species reads where the blast mapping and bwa mapping positions disagree or where blast is unable to determine conclusively where the read maps (i.e.E-scores are the same). There is a step which provides an option to recover those reads by using a user-generated bash script.  Currently, we use a bash script to call a python script which will actually accomplish the recovery, but this functionality may be changed in the future.  In general, these scripts must:</p>
<ol>
<li>accept ambiguous reads as $1</li>
<li>accept non-ambiguous reads as $2</li>
<li>take a file name for output reads as $3</li>
<li>take a basePath for location of script files as $4</li>
</ol>
<p>All script files must be stored in scripts/RecoveryScripts.</p>
<h2>
<a id="user-content-9-running-pipelines" class="anchor" href="#9-running-pipelines" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>9: Running pipelines:</h2>
<p>The pipeline can be run using the DS command created by the setup script:</p>
<div class="highlight highlight-source-shell"><pre>DS CONFIG_CSV.csv</pre></div>
<h2>
<a id="user-content-10-output-file-descriptions" class="anchor" href="#10-output-file-descriptions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>10: Output file descriptions:</h2>
<p>The pipeline will create a set of summary files covering all samples, as well as a file directory structure for each sample.  The summary files are:</p>
<table>
<thead>
<tr>
<th>File Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>summary.csv</td>
<td>A csv file with summary metrics for all samples.</td>
</tr>
<tr>
<td>summaryDepth.pdf</td>
<td>A pdf file containing depth per target plots for all samples.</td>
</tr>
<tr>
<td>summaryFamilySize.pdf</td>
<td>A pdf file containing family size plots for all samples.</td>
</tr>
<tr>
<td>summaryInsertSize.pdf</td>
<td>A pdf file containing insert size plots for all samples.</td>
</tr>
<tr>
<td>summaryMutsByCycle.pdf</td>
<td>A pdf file containing non-SNP mutations per cycle for all samples.</td>
</tr>
</tbody>
</table>
<p>This directory structure looks like this:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">.</span>
└── SAMP_DIR
   ├── Final
   │   ├── dcs
   │   │   ├── SAMPLE.dcs.countmuts.txt
   │   │   ├── SAMPLE.dcs.final.bam
   │   │   ├── SAMPLE.dcs.final.bam.bai
   │   │   ├── SAMPLE.dcs.mutated.bam
   │   │   ├── SAMPLE.dcs.mutated.bam.bai
   │   │   ├── SAMPLE.dcs.snps.vcf
   │   │   └── SAMPLE.dcs.vcf
   │   ├── SAMPLE.report.html
   │   └── sscs
   │       ├── SAMPLE.sscs.final.bam
   │       └── SAMPLE.sscs.final.bam.bai
   ├── Intermediate
   │   ├── ConsensusMakerOutputs
   │   │   ├── SAMPLE_aln_seq1.fq.gz
   │   │   ├── SAMPLE_aln_seq2.fq.gz
   │   │   ├── SAMPLE_read1_dcs.fq.gz
   │   │   ├── SAMPLE_read1_sscs.fq.gz
   │   │   ├── SAMPLE_read2_dcs.fq.gz
   │   │   └── SAMPLE_read2_sscs.fq.gz
   │   ├── postBlast
   │   │   ├── FilteredReads
   │   │   │   ├── SAMPLE_dcs.ambig.sort.bam
   │   │   │   ├── SAMPLE_dcs.ambig.sort.bam.bai
   │   │   │   ├── SAMPLE_dcs.wrongSpecies.sort.bam
   │   │   │   └── SAMPLE_dcs.wrongSpecies.sort.bam.bai
   │   │   ├── SAMPLE_dcs.blast.xml
   │   │   ├── SAMPLE_dcs.preBlast.mutated.bam
   │   │   └── SAMPLE_dcs.preBlast.unmutated.bam
   │   └── PreVariantCallsCp
   │       ├── dcs
   │       │   ├── SAMPLE.dcs.clipped.bai
   │       │   └── SAMPLE.dcs.clipped.bam
   │       └── sscs
   │           ├── SAMPLE.sscs.clipped.bai
   │           └── SAMPLE.sscs.clipped.bam
   ├── logs
   │   └── Log Files
   ├── SAMPLE_config.sh
   ├── SAMPLE_seq1.fastq.gz
   ├── SAMPLE_seq2.fastq.gz
   └── Stats
       ├── data
       │   ├── SAMPLE_cmStats.txt
       │   ├── SAMPLE.dcs_ambiguity_counts.txt
       │   ├── SAMPLE.dcs.clipped.metrics.txt
       │   ├── SAMPLE.dcs.filt.clipped.metrics.txt
       │   ├── SAMPLE.dcs.filt.no_overlap.metrics.txt
       │   ├── SAMPLE.dcs.iSize_Metrics.txt
       │   ├── SAMPLE.dcs_MutsPerCycle.dat.csv
       │   ├── SAMPLE.dcs.mutsPerRead.txt
       │   ├── SAMPLE.dcs.region.mutpos.vcf_depth.txt
       │   ├── SAMPLE.dcs.region.snpFiltered.mutsPerRead.txt
       │   ├── SAMPLE_dcs.speciesComp.txt
       │   ├── SAMPLE_mem.dcs.sort.flagstats.txt
       │   ├── SAMPLE_mem.sscs.sort.flagstats.txt
       │   ├── SAMPLE_onTargetCount.txt
       │   ├── SAMPLE.sscs.clipped.metrics.txt
       │   ├── SAMPLE.sscs.filt.clipped.metrics.txt
       │   ├── SAMPLE.sscs.filt.no_overlap.metrics.txt
       │   ├── SAMPLE.tagstats.txt
       │   └── SAMPLE.temp.sort.flagstats.txt
       ├── SAMPLE.report.ipynb
       └── plots
           ├── SAMPLE.dcs_BasePerPosInclNs.png
           ├── SAMPLE.dcs_BasePerPosWithoutNs.png
           ├── SAMPLE.dcs.iSize_Histogram.png
           ├── SAMPLE.dcs.mutsPerRead.png
           ├── SAMPLE.dcs.region.snpFiltered.mutsPerRead.png
           ├── SAMPLE.dcs.targetCoverage.png
           ├── SAMPLE_family_size.png
           └── SAMPLE_fam_size_relation.png</pre></div>
<p>File descriptions are as follows:</p>
<table>
<thead>
<tr>
<th>Directory</th>
<th>File name</th>
<th>Description</th>
<th>When Generated</th>
</tr>
</thead>
<tbody>
<tr>
<td>.</td>
<td>SAMPLE_config.sh</td>
<td>Per-sample config file, showing run parameters for this sample.</td>
<td>Always</td>
</tr>
<tr>
<td>.</td>
<td>SAMPLE_seq1.fastq.gz</td>
<td>Input read 1 file</td>
<td>Input</td>
</tr>
<tr>
<td>.</td>
<td>SAMPLE_seq2.fastq.gz</td>
<td>Input read 2 file</td>
<td>Input</td>
</tr>
<tr>
<td>.</td>
<td>Final</td>
<td>Directory containing final bam and vcf files</td>
<td>Always</td>
</tr>
<tr>
<td>Final</td>
<td>dcs</td>
<td>Directory containing final dcs files</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.countmuts.txt</td>
<td>Countmuts file, showing a summary of mutation data for DCS reads.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.final.bam</td>
<td>Final file for DCS reads, including all reads that overlap the bed file.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.final.bam.bai</td>
<td>Index for final DCS reads.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.mutated.bam</td>
<td>File containing DCS reads with non-SNP mutations</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.mutated.bam.bai</td>
<td>Index for DCS mutated reads file.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.snps.vcf</td>
<td>VCF file containing SNPs overlapping bed file in DCS.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/dcs</td>
<td>SAMPLE.dcs.vcf</td>
<td>VCF file containing all variants overlapping bed file in DCS.</td>
<td>Always</td>
</tr>
<tr>
<td>Final</td>
<td>SAMPLE.report.html</td>
<td>Summary report for this sample</td>
<td>Always</td>
</tr>
<tr>
<td>Final</td>
<td>sscs</td>
<td>Directory containing final SSCS files</td>
<td>Always</td>
</tr>
<tr>
<td></td>
<td>SAMPLE.sscs.countmuts.txt</td>
<td>Countmuts file, showing a summary of mutation data for SSCS reads.</td>
<td></td>
</tr>
<tr>
<td>Final/sscs</td>
<td>SAMPLE.sscs.final.bam</td>
<td>Final file for SSCS reads, including all reads that overlap the bed file.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/sscs</td>
<td>SAMPLE.sscs.final.bam.bai</td>
<td>Index for final SSCS reads.</td>
<td>Always</td>
</tr>
<tr>
<td>Final/sscs</td>
<td>SAMPLE.sscs.mutated.bam</td>
<td>File containing SSCS reads with non-SNP mutations</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Final/sscs</td>
<td>SAMPLE.sscs.mutated.bam.bai</td>
<td>Index for SSCS mutated reads file.</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Final/sscs</td>
<td>SAMPLE.sscs.snps.vcf</td>
<td>VCF file containing SNPs overlapping bed file in SSCS.</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Final/sscs</td>
<td>SAMPLE.sscs.vcf</td>
<td>VCF file containing all variants overlapping bed file in SSCS.</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>.</td>
<td>Intermediate</td>
<td>Directory containing intermediate checkpointing files</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate</td>
<td>ConsensusMakerOutputs</td>
<td>Directory for post-consensus maker checkpoint files</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/ConsensusMakerOutputs</td>
<td>SAMPLE_aln_seq1.fq.gz</td>
<td>Read 1 file for raw on-target determination</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/ConsensusMakerOutputs</td>
<td>SAMPLE_aln_seq2.fq.gz</td>
<td>Read 2 file for raw on-target determination</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/ConsensusMakerOutputs</td>
<td>SAMPLE_read1_dcs.fq.gz</td>
<td>DCS Read 1 File</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/ConsensusMakerOutputs</td>
<td>SAMPLE_read1_sscs.fq.gz</td>
<td>SSCS Read 1 file</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/ConsensusMakerOutputs</td>
<td>SAMPLE_read2_dcs.fq.gz</td>
<td>DCS Read 2 File</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/ConsensusMakerOutputs</td>
<td>SAMPLE_read2_sscs.fq.gz</td>
<td>SSCS Read 2 file</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate</td>
<td>postBlast</td>
<td>Directory for post-BLAST checkpoint files.  Only affects DCS.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast</td>
<td>FilteredReads</td>
<td>Directory for reads that got filtered out of DCS processing due to BLAST analysis indicating that they were either the wrong species or ambiguously mapped.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast/FilteredReads</td>
<td>SAMPLE_dcs.ambig.sort.bam</td>
<td>Reads that were filtered out due to ambiguous mapping according to BLAST alignment.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast/FilteredReads</td>
<td>SAMPLE_dcs.ambig.sort.bam.bai</td>
<td>Index for ambiguous reads file</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast/FilteredReads</td>
<td>SAMPLE_dcs.wrongSpecies.sort.bam</td>
<td>Reads that were filtered out due to BLAST alignment indicating that they were from the wrong species, or where species of origin could not be determined.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast/FilteredReads</td>
<td>SAMPLE_dcs.wrongSpecies.sort.bam.bai</td>
<td>Index for wrong-species file</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast</td>
<td>SAMPLE_dcs.blast.xml</td>
<td>BLAST xml output</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast</td>
<td>SAMPLE_dcs.preBlast.mutated.bam</td>
<td>DCS with potential non-SNP variants that were submitted to BLAST.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/postBlast</td>
<td>SAMPLE_dcs.preBlast.unmutated.bam</td>
<td>DCS reads without non-SNP variants.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate</td>
<td>PreVariantCallsCp</td>
<td>Directory for final checkpoint, immediately before variant calling.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/PreVariantCallsCp</td>
<td>dcs</td>
<td>Directory containing final checkpoint DCS bam files</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/PreVariantCallsCp/dcs</td>
<td>SAMPLE.dcs.clipped.bai</td>
<td>Final file containing all DCS reads, including those that don’t overlap the bed file.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/PreVariantCallsCp/dcs</td>
<td>SAMPLE.dcs.clipped.bam</td>
<td>Final file containing all DCS reads, including those that don’t overlap the bed file.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/PreVariantCallsCp</td>
<td>sscs</td>
<td>Directory containing final checkpoint SSCS bam files</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/PreVariantCallsCp/sscs</td>
<td>SAMPLE.sscs.clipped.bai</td>
<td>Index for final file containing all SSCS reads, including those that don’t overlap the bed file.</td>
<td>Always</td>
</tr>
<tr>
<td>Intermediate/PreVariantCallsCp/sscs</td>
<td>SAMPLE.sscs.clipped.bam</td>
<td>Final file containing all SSCS reads, including those that don’t overlap the bed file.</td>
<td>Always</td>
</tr>
<tr>
<td>.</td>
<td>logs</td>
<td>Directory containing log files for this sample.</td>
<td>Always</td>
</tr>
<tr>
<td>.</td>
<td>Stats</td>
<td>Directory containing statistics files</td>
<td>Always</td>
</tr>
<tr>
<td>Stats</td>
<td>data</td>
<td>Directory containing statistics data files.</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE_cmStats.txt</td>
<td>Statistics from the Consensus Maker</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs_ambiguity_counts.txt</td>
<td>Statistics on ambiguity counts</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs.clipped.metrics.txt</td>
<td>Statistics on fixed end clipping in DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs.filt.clipped.metrics.txt</td>
<td>Statistics on overlap clipping in DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs.iSize_Metrics.txt</td>
<td>Statistics on insert size in DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs_MutsPerCycle.dat.csv</td>
<td>Statistics file for non-SNP mutations per cycle in DCS reads</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs.mutsPerRead.txt</td>
<td>Statistics file for non-SNP mutations per read in DCS reads</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.dcs.region.mutpos.vcf_depth.txt</td>
<td>Per-base coverage and N counts for final DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE_dcs.speciesComp.txt</td>
<td>File containing species assignment data for DCS reads</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE_mem.dcs.sort.flagstats.txt</td>
<td>Initial alignment statistics for DCS reads</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE_mem.sscs.sort.flagstats.txt</td>
<td>Initial alignment statistics for SSCS reads</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE_onTargetCount.txt</td>
<td>Raw on target statistics</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.sscs.clipped.metrics.txt</td>
<td>Statistics on fixed end clipping in SSCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.sscs.filt.clipped.metrics.txt</td>
<td>Statistics on overlap clipping in SSCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.sscs_MutsPerCycle.dat.csv</td>
<td>Text data of error rate per cycle in unclipped SSCS</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.sscs.mutsPerRead.txt</td>
<td>Statistics file for non-SNP mutations per read in SSCS reads</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.sscs.region.mutpos.vcf_depth.txt</td>
<td>Per-base coverage and N counts for final SSCS</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.tagstats.txt</td>
<td>Family size data (in text form)</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/data</td>
<td>SAMPLE.temp.sort.flagstats.txt</td>
<td>Statistics on initial read counts</td>
<td>Always</td>
</tr>
<tr>
<td>Stats</td>
<td>SAMPLE.report.ipynb</td>
<td>iPython notebook for the HTML report</td>
<td>Always</td>
</tr>
<tr>
<td>Stats</td>
<td>plots</td>
<td>Directory containing statistics plots.</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.dcs_BasePerPosInclNs.png</td>
<td>Plot of error rate per cycle for DCS, including Ns</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.dcs_BasePerPosWithoutNs.png</td>
<td>Plot of error rate per cycle for DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.dcs.iSize_Histogram.png</td>
<td>Histogram of insert size metrics for un-clipped DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.dcs.mutsPerRead.png</td>
<td>Plot of mutations per read in DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.dcs.targetCoverage.png</td>
<td>Plot of per-target coverage in DCS</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE_family_size.png</td>
<td>Plot of family size distribution</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE_fam_size_relation.png</td>
<td>Plot of relationship between a:b and b:a families</td>
<td>Always</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.sscs_BasePerPosInclNs.png</td>
<td>Plot of error rate per cycle SSCS, including Ns</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.sscs_BasePerPosWithoutNs.png</td>
<td>Plot of error rate per cycle for SSCS</td>
<td>runSscs=True</td>
</tr>
<tr>
<td>Stats/plots</td>
<td>SAMPLE.sscs.mutsPerRead.png</td>
<td>Plot of mutations per read in SSCS</td>
<td>runSscs=True</td>
</tr>
</tbody>
</table>
<h2>
<a id="user-content-11-testing-the-pipeline" class="anchor" href="#11-testing-the-pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>11: Testing the pipeline</h2>
<p>The newly setup pipeline can be tested using provided data and files located in the 'test' directory.
To test the pipeline, change into the 'test' directory and invoking at the command prompt:</p>
<div class="highlight highlight-source-shell"><pre>DS testConfig.csv</pre></div>
<p>A final expected output report can be found in the testData/Final directory and be compared to the expected_report.html file
located in the parent test directory.</p>
<h2>
<a id="user-content-12-full-and-partial-reruns" class="anchor" href="#12-full-and-partial-reruns" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>12: Full and partial reruns</h2>
<p>Sometimes it may be necessary to rerun all or part of the pipeline for various reasons.  The following table lists some of the reasons you might want to rerun all or part of the pipeline, how much of the pipeline you want to rerun in those cases, and how to carry out the rerun.</p>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Amount to rerun</th>
<th>Preparation steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wrong bed file used</td>
<td>From pre-variant calling</td>
<td>Remove the following files: <ul>
<li>summary.csv</li>
<li>summaryMutsByCycle.pdf</li>
<li>summaryDepth.pdf</li>
<li>summaryFamilySize.pdf</li>
<li>summaryInsertSize.pdf</li>
<li>All _config.sh files for samples that you want to force a rerun of</li>
</ul>Remove the following directories: <ul><li>All Final directories in samples that you want to force a rerun of.  </li></ul>
</td>
</tr>
<tr>
<td><ul>
<li>Wrong clipping parameters used</li>
<li>Wrong target taxon ID used</li>
</ul></td>
<td>From post-blast</td>
<td>Remove the following files: <ul>
<li>summary.csv</li>
<li>summaryMutsByCycle.pdf</li>
<li>summaryDepth.pdf</li>
<li>summaryFamilySize.pdf</li>
<li>summaryInsertSize.pdf</li>
<li>All _config.sh files for samples that you want to force a rerun of</li>
</ul>Remove the following directories: <ul>
<li>All Final directories in samples that you want to force a rerun of.  </li>
<li>All Intermediate/PreVariantCallsCp directories in samples that you want to force a rerun of</li>
</ul>
</td>
</tr>
<tr>
<td><ul>
<li>Wrong contaminant db used</li>
<li>Wrong reference genome used</li>
</ul></td>
<td>From post-Consensus Maker</td>
<td>Remove the following files: <ul>
<li>summary.csv</li>
<li>summaryMutsByCycle.pdf</li>
<li>summaryDepth.pdf</li>
<li>summaryFamilySize.pdf</li>
<li>summaryInsertSize.pdf</li>
<li>All _config.sh files for samples that you want to force a rerun of</li>
</ul>Remove the following directories: <ul>
<li>All Final directories in samples that you want to force a rerun of.  </li>
<li>All Intermediate/PreVariantCallsCp directories in samples that you want to force a rerun of</li>
<li>All Intermediate/postBlast directories in samples that you want to force a rerun of</li>
</ul>
</td>
</tr>
<tr>
<td>Wrong consensus making parameters used</td>
<td>From beginning</td>
<td>Remove the following files: <ul>
<li>summary.csv</li>
<li>summaryMutsByCycle.pdf</li>
<li>summaryDepth.pdf</li>
<li>summaryFamilySize.pdf</li>
<li>summaryInsertSize.pdf</li>
<li>All _config.sh files for samples that you want to force a rerun of</li>
</ul>Remove the following directories: <ul>
<li>All Final directories in samples that you want to force a rerun of.  </li>
<li>All Intermediate directories in samples that you want to force a rerun of</li>
<li>All Stats directories in samples that you want to force a rerun of</li>
</ul>
</td>
</tr>
</tbody>
</table>

              </article>
            </div>
          </div>
        </div>
      </div>

    

  </div>
  <div>&nbsp;</div>
  </div><script>
    function showCanonicalImages() {
      var images = document.getElementsByTagName('img');
      if (!images) {
        return;
      }
      for (var index = 0; index < images.length; index++) {
        var image = images[index];
        if (image.getAttribute('data-canonical-src') && image.src !== image.getAttribute('data-canonical-src')) {
          image.src = image.getAttribute('data-canonical-src');
        }
      }
    }

    function scrollToHash() {
      if (location.hash && !document.querySelector(':target')) {
        var element = document.getElementById('user-content-' + location.hash.slice(1));
        if (element) {
           element.scrollIntoView();
        }
      }
    }

    function autorefreshContent(eventSourceUrl) {
      var initialTitle = document.title;
      var contentElement = document.getElementById('grip-content');
      var source = new EventSource(eventSourceUrl);
      var isRendering = false;

      source.onmessage = function(ev) {
        var msg = JSON.parse(ev.data);
        if (msg.updating) {
          isRendering = true;
          document.title = '(Rendering) ' + document.title;
        } else {
          isRendering = false;
          document.title = initialTitle;
          contentElement.innerHTML = msg.content;
          showCanonicalImages();
        }
      }

      source.onerror = function(e) {
        if (e.readyState === EventSource.CLOSED && isRendering) {
          isRendering = false;
          document.title = initialTitle;
        }
      }
    }

    window.onhashchange = function() {
      scrollToHash();
    }

    window.onload = function() {
      scrollToHash();
    }

    showCanonicalImages();

    var autorefreshUrl = document.getElementById('preview-page').getAttribute('data-autorefresh-url');
    if (autorefreshUrl) {
      autorefreshContent(autorefreshUrl);
    }
  </script>
</body>
</html>